{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Note\n",
    "\n",
    "## 1. Image classification\n",
    "\n",
    "### Challenges:\n",
    "\n",
    "- Viewpoint variation: A single instance of an object can be oriented in many ways with respect to the camera.\n",
    "- Scale variation\n",
    "- Deformation: Many objects of interest are not rigid bodies and can be\n",
    "  deformed in extreme ways\n",
    "- Occlusion: The objects of interest can be occluded. Sometimes only a\n",
    "  small portion of an object (as little as few pixels) could be visible\n",
    "- Illumination conditions: The effects of illumination are drastic on the pixel level.\n",
    "- Background clutter: The objects of interest may blend into their environment, making them hard to identify.\n",
    "- Intra-class variation: The classes of interest can often be relatively broad, such as chairs. There are many different types of these objects, each with their own appearance\n",
    "\n",
    "### Data-driven approach:\n",
    "\n",
    "- Relies on first accumulating a *training dataset* of labeled images.\n",
    "\n",
    "### CIFAR-10\n",
    "\n",
    "- 60,000 images with size 32x32, 10 classes.\n",
    "- Training set: 50,000\n",
    "- Test set: 10,000\n",
    "\n",
    "### Nearest Neighbor Classifier\n",
    "\n",
    "- Take a test image, compare it to every single one of the training images,\n",
    "  and predict the label of the closest training image.\n",
    "- The choices of distance: $L1$, $L2$ , ...\n",
    "\n",
    "### k-Nearest Neighbor Classifier\n",
    "\n",
    "- Instead of finding the single closest image in the training set, we will find the top **k** closest images and have them vote on the label of the test image.\n",
    "\n",
    "### Validation sets for Hyperparameter tuning\n",
    "\n",
    "- Split training set\n",
    "- Cross-validation: Instead of arbitrarily picking the first 1000 data points to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of $k$ /> works by iterating over different validation sets and averaging the performance across these.\n",
    "\n",
    "### Pros and Cons of Nearest Neighbor classifier\n",
    "\n",
    "#### Advantage\n",
    "\n",
    "- Simple to implement and understand\n",
    "- Classifier takes no time to train\n",
    "\n",
    "#### Disadvantage\n",
    "\n",
    "- Computational cost at test time\n",
    "- Distances over high-dimensional spaces can be very counter-intuitive\n",
    "\n",
    "## 2. Linear Classification\n",
    "\n",
    "### Linear mapping\n",
    "\n",
    "- $f(x_i,W,b)=Wx_i+b$\n",
    "- The single matrix multiplication $W{x_i}$ is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of $W$.\n",
    "\n",
    "### Interpreting a linear classifier\n",
    "\n",
    "**As high-dimensional points**\n",
    "\n",
    "- Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space\n",
    "- Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space\n",
    "\n",
    "**As template matching**\n",
    "\n",
    "- Each row of <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" /> corresponds to a *template* (or sometimes also called a *prototype*) for one of the classes\n",
    "- The score of each class for an image is then obtained by comparing each template with the image using an *inner product* (or *dot product*) one by one to find the one that “fits” best.\n",
    "  **Bias trick**\n",
    "- $f(x_i,W,b)=Wx_i+b$ -> $f(x_i,W)=Wx_i$\n",
    "\n",
    "### Loss function\n",
    "\n",
    "#### mSVM loss\n",
    "\n",
    "- The score function takes the pixels and computes the vector $f(x_i,W)$ of class scores, which we will abbreviate to $s$ (short for scores).\n",
    "- $L_i= \\sum_{j≠yi} max( 0, s_j-s_{y_i} + \\Delta )$\n",
    "- In linear classifier:\n",
    "\n",
    "$Li=\\sum_{j≠yi} max( 0, w^T_{j}x_{i}-w^T_{y_i}x_i+ \\Delta )$\n",
    "\n",
    "$w_i$ is the i_th row of $W$ reshaped as a column\n",
    "\n",
    "- The theshold at zero $max(0,-)$ function is called **hingle loss**\n",
    "- **Squared hingle loss SVM** (L2-SVM): $max(0,-)^2$\n",
    "- Regularization:\n",
    "  - There might be many similar **W** that correctly classify the examples\n",
    "  - if some parameters **W** correctly classify all examples (so loss is zero for each example), then any multiple of these parameters $λW$ where $λ$ will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences\n",
    "  - -> Extending the loss function with a **regularization penalty** $R(W)$\n",
    "  - $L2$ penalty:\n",
    "\n",
    "$R(W) = \\sum_k\\sum_lW^2_{k,l}$\n",
    "\n",
    "- Loss becomes: $L = \\frac{1}{N}\\sum_iL_i + \\lambda R(W)$\n",
    "\n",
    "- Max margin property in SVM: **CS229**\n",
    "- Generalization property -> less overfitting\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- Setting <img src=\"https://i.upmath.me/svg/%5CDelta\" alt=\"\\Delta\" />:\n",
    "  - This hyperparameter can safely be set to <img src=\"https://i.upmath.me/svg/%CE%94%3D1.0\" alt=\"Δ=1.0\" /> in all cases.\n",
    "  - The hyperparameters <img src=\"https://i.upmath.me/svg/%5CDelta\" alt=\"\\Delta\" /> and <img src=\"https://i.upmath.me/svg/%CE%BB\" alt=\"λ\" /> seem like two different hyperparameters, but in fact, they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective\n",
    "  - The exact value of the margin between the scores (e.g. <img src=\"https://i.upmath.me/svg/%CE%94%3D1\" alt=\"Δ=1\" />, or <img src=\"https://i.upmath.me/svg/%CE%94%3D100\" alt=\"Δ=100\" />) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily.\n",
    "\n",
    "### Softmax classifier\n",
    "\n",
    "- Cross-entropy loss:\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/%0A%20%20L_i%20%3D%20-log(%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j%20e%5E%7Bf_j%7D%7D)%20%20%3D%20-f_%7By_i%7D%20%2B%20log%5Csum_j%20e%5E%7Bf_j%7D%0A%20%20\" alt=\"\n",
    "L_i = -log(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}})  = -f_{y_i} + log\\sum_j e^{f_j}\n",
    "\" />\n",
    "\n",
    "- <img src=\"https://i.upmath.me/svg/f_j\" alt=\"f_j\" /> is the j-th element of the vector of class scores <img src=\"https://i.upmath.me/svg/f\" alt=\"f\" />\n",
    "\n",
    "#### Information theory view:\n",
    "\n",
    "- The cross-entropy between a 'true' distribution <img src=\"https://i.upmath.me/svg/p\" alt=\"p\" /> and an estimated distribution <img src=\"https://i.upmath.me/svg/q\" alt=\"q\" /> is defined as:\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/%0A%20%20H(p%2Cq)%20%3D%20-%5Csum_x%20p(x)%20log%20q(x)%0A%20%20\" alt=\"\n",
    "H(p,q) = -\\sum_x p(x) log (q(x))\n",
    "\" />\n",
    "\n",
    "- The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities (<img src=\"https://i.upmath.me/svg/q%3D%5Cfrac%7Be%5E%7Bf_%7Byi%7D%7D%7D%7B%E2%88%91_j%20e%5E%7Bf_j%7D%7D\" alt=\"q=\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\" />\n",
    "  as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e.<img src=\"https://i.upmath.me/svg/p%3D%5B0%2C%E2%80%A61%2C%E2%80%A6%2C0%5D\" alt=\"p=[0,…1,…,0]\" /> contains a single <img src=\"https://i.upmath.me/svg/%201\" alt=\" 1\" /> at the <img src=\"https://i.upmath.me/svg/y_i\" alt=\"y_i\" /> <img src=\"https://i.upmath.me/svg/i-th\" alt=\"i-th\" /> position.)\n",
    "- Since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as <img src=\"https://i.upmath.me/svg/H(p%2Cq)%3DH(p)%2BDKL(p%7C%7Cq)\" alt=\"H(p,q)=H(p)+DKL(p||q)\" />, and the entropy of the delta function pp is zero, this is also equivalent to minimizing the KL divergence between the two distributions.\n",
    "\n",
    "#### Probabilistic Interpretation\n",
    "\n",
    "- We can see\n",
    "  <img src=\"https://i.upmath.me/svg/%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j%20e%5E%7Bf_j%7D%7D\" alt=\"\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\" /> as the normalized probability assigned to the correct label <img src=\"https://i.upmath.me/svg/y_i\" alt=\"y_i\" /> given the image <img src=\"https://i.upmath.me/svg/x_i\" alt=\"x_i\" />\n",
    "- In the probabilistic interpretation, we are therefore minimizing the negative log-likelihood of the correct class, which can be interpreted as performing *Maximum Likelihood Estimation* (MLE)\n",
    "- A nice feature of this view is that we can now also interpret the regularization term <img src=\"https://i.upmath.me/svg/R(W)\" alt=\"R(W)\" /> in the full loss function as coming from a Gaussian prior over the weight matrix <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" />, where instead of MLE we are performing the *Maximum a posteriori* (MAP) estimation\n",
    "\n",
    "### SVM vs Softmax\n",
    "\n",
    "#### Softmax classifier provides probabilities for each class\n",
    "\n",
    "#### SVM and Softmax are usually comparable\n",
    "\n",
    "- Compared to the Softmax classifier, the SVM is a more *local* objective\n",
    "  - The SVM does not care about the details of the individual scores: if they were instead [10, -100, -100] or [10, 9, 9] the SVM would be indifferent since the margin of 1 is satisfied and hence the loss is zero\n",
    "- The Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better\n",
    "\n",
    "## 3. Optimization\n",
    "\n",
    "### Optimization\n",
    "\n",
    "#### Random search\n",
    "\n",
    "- Try random weights and keep track of what works best\n",
    "\n",
    "#### Random local search\n",
    "\n",
    "- We will start out with a random <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" />, generate random perturbations <img src=\"https://i.upmath.me/svg/%5CdeltaW\" alt=\"\\deltaW\" /> to it and if the loss at the perturbed <img src=\"https://i.upmath.me/svg/W%20%2B%20%5Cdelta%20W\" alt=\"W + \\delta W\" /> is lower, we will perform an update\n",
    "\n",
    "#### Following the gradient\n",
    "\n",
    "- It turns out that there is no need to randomly search for a good direction: we can compute the *best* direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descend\n",
    "\n",
    "### Computing the gradient\n",
    "\n",
    "#### Computing the gradient numerically with finite differences\n",
    "Note\n",
    "\n",
    "## 1. Image classification\n",
    "\n",
    "### Challenges:\n",
    "\n",
    "- Viewpoint variation: A single instance of an object can be oriented in many ways with respect to the camera.\n",
    "- Scale variation\n",
    "- Deformation: Many objects of interest are not rigid bodies and can be\n",
    "  deformed in extreme ways\n",
    "- Occlusion: The objects of interest can be occluded. Sometimes only a\n",
    "  small portion of an object (as little as few pixels) could be visible\n",
    "- Illumination conditions: The effects of illumination are drastic on the pixel level.\n",
    "- Background clutter: The objects of interest may blend into their environment, making them hard to identify.\n",
    "- Intra-class variation: The classes of interest can often be relatively broad, such as chairs. There are many different types of these objects, each with their own appearance\n",
    "\n",
    "### Data-driven approach:\n",
    "\n",
    "- Relies on first accumulating a *training dataset* of labeled images.\n",
    "\n",
    "### CIFAR-10\n",
    "\n",
    "- 60,000 images with size 32x32, 10 classes.\n",
    "- Training set: 50,000\n",
    "- Test set: 10,000\n",
    "\n",
    "### Nearest Neighbor Classifier\n",
    "\n",
    "- Take a test image, compare it to every single one of the training images,\n",
    "  and predict the label of the closest training image.\n",
    "- The choices of distance: <img src=\"https://i.upmath.me/svg/L1\" alt=\"L1\" />, <img src=\"https://i.upmath.me/svg/L2\" alt=\"L2\" />, ...\n",
    "\n",
    "### k-Nearest Neighbor Classifier\n",
    "\n",
    "- Instead of finding the single closest image in the training set, we will find the top **k** closest images and have them vote on the label of the test image.\n",
    "\n",
    "### Validation sets for Hyperparameter tuning\n",
    "\n",
    "- Split training set\n",
    "- Cross-validation: Instead of arbitrarily picking the first 1000 data points to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of <img src=\"https://i.upmath.me/svg/k\" alt=\"k\" /> works by iterating over different validation sets and averaging the performance across these.\n",
    "\n",
    "### Pros and Cons of Nearest Neighbor classifier\n",
    "\n",
    "#### Advantage\n",
    "\n",
    "- Simple to implement and understand\n",
    "- Classifier takes no time to train\n",
    "\n",
    "#### Disadvantage\n",
    "\n",
    "- Computational cost at test time\n",
    "- Distances over high-dimensional spaces can be very counter-intuitive\n",
    "\n",
    "## 2. Linear Classification\n",
    "\n",
    "### Linear mapping\n",
    "\n",
    "- <img src=\"https://i.upmath.me/svg/f(x_i%2CW%2Cb)%3DWx_i%2Bb\" alt=\"f(x_i,W,b)=Wx_i+b\" />\n",
    "- The single matrix multiplication <img src=\"https://i.upmath.me/svg/W%7Bx_i%7D\" alt=\"W{x_i}\" /> is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" />.\n",
    "\n",
    "### Interpreting a linear classifier\n",
    "\n",
    "**As high-dimensional points**\n",
    "\n",
    "- Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space\n",
    "- Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space\n",
    "\n",
    "**As template matching**\n",
    "\n",
    "- Each row of <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" /> corresponds to a *template* (or sometimes also called a *prototype*) for one of the classes\n",
    "- The score of each class for an image is then obtained by comparing each template with the image using an *inner product* (or *dot product*) one by one to find the one that “fits” best.\n",
    "  **Bias trick**\n",
    "- <img src=\"https://i.upmath.me/svg/f(x_i%2CW%2Cb)%3DWx_i%2Bb\" alt=\"f(x_i,W,b)=Wx_i+b\" /> -> <img src=\"https://i.upmath.me/svg/f(x_i%2CW)%3DWx_i\" alt=\"f(x_i,W)=Wx_i\" />\n",
    "\n",
    "### Loss function\n",
    "\n",
    "#### mSVM loss\n",
    "\n",
    "- The score function takes the pixels and computes the vector <img src=\"https://i.upmath.me/svg/f(x_i%2CW)\" alt=\"f(x_i,W)\" /> of class scores, which we will abbreviate to <img src=\"https://i.upmath.me/svg/s\" alt=\"s\" /> (short for scores).\n",
    "- <p class=\"aligncenter\"> <img src=\"https://i.upmath.me/svg/L_i%3D%20%5Csum_%7Bj%E2%89%A0yi%7D%20max(%200%2C%20s_j-s_%7By_i%7D%20%2B%20%5CDelta%20)\" alt=\"L_i= \\sum_{j≠yi} max( 0, s_j-s_{y_i} + \\Delta )\" /> </p>\n",
    "- In linear classifier:\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/Li%3D%5Csum_%7Bj%E2%89%A0yi%7D%20max(%200%2C%20w%5ET_%7Bj%7Dx_%7Bi%7D-w%5ET_%7By_i%7Dx_i%2B%20%5CDelta%20)\" alt=\"Li=\\sum_{j≠yi} max( 0, w^T_{j}x_{i}-w^T_{y_i}x_i+ \\Delta )\" />\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/w_i\" alt=\"w_i\" /> is the i_th row of <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" /> reshaped as a column\n",
    "\n",
    "- The theshold at zero <img src=\"https://i.upmath.me/svg/max(0%2C-)\" alt=\"max(0,-)\" /> function is called **hingle loss**\n",
    "- **Squared hingle loss SVM** (L2-SVM): <img src=\"https://i.upmath.me/svg/max(0%2C-)%5E2\" alt=\"max(0,-)^2\" />\n",
    "- Regularization:\n",
    "  - There might be many similar **W** that correctly classify the examples\n",
    "  - if some parameters **W** correctly classify all examples (so loss is zero for each example), then any multiple of these parameters <img src=\"https://i.upmath.me/svg/%CE%BBW\" alt=\"λW\" /> where <img src=\"https://i.upmath.me/svg/%CE%BB%3E1\" alt=\"λ&gt;1\" /> will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences\n",
    "  - -> Extending the loss function with a **regularization penalty** <img src=\"https://i.upmath.me/svg/R(W)\" alt=\"R(W)\" />\n",
    "  - <img src=\"https://i.upmath.me/svg/L2\" alt=\"L2\" /> penalty:\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/R(W)%20%3D%20%5Csum_k%5Csum_lW%5E2_%7Bk%2Cl%7D\" alt=\"R(W) = \\sum_k\\sum_lW^2_{k,l}\" />\n",
    "\n",
    "- Loss becomes:\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/L%20%3D%20%5Cfrac%7B1%7D%7BN%7D%5Csum_iL_i%20%2B%20%5Clambda%20R(W)\" alt=\"L = \\frac{1}{N}\\sum_iL_i + \\lambda R(W)\" />.\n",
    "\n",
    "- Max margin property in SVM: **CS229**\n",
    "- Generalization property -> less overfitting\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- Setting <img src=\"https://i.upmath.me/svg/%5CDelta\" alt=\"\\Delta\" />:\n",
    "  - This hyperparameter can safely be set to <img src=\"https://i.upmath.me/svg/%CE%94%3D1.0\" alt=\"Δ=1.0\" /> in all cases.\n",
    "  - The hyperparameters <img src=\"https://i.upmath.me/svg/%5CDelta\" alt=\"\\Delta\" /> and <img src=\"https://i.upmath.me/svg/%CE%BB\" alt=\"λ\" /> seem like two different hyperparameters, but in fact, they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective\n",
    "  - The exact value of the margin between the scores (e.g. <img src=\"https://i.upmath.me/svg/%CE%94%3D1\" alt=\"Δ=1\" />, or <img src=\"https://i.upmath.me/svg/%CE%94%3D100\" alt=\"Δ=100\" />) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily.\n",
    "\n",
    "### Softmax classifier\n",
    "\n",
    "- Cross-entropy loss:\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/%0A%20%20L_i%20%3D%20-log(%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j%20e%5E%7Bf_j%7D%7D)%20%20%3D%20-f_%7By_i%7D%20%2B%20log%5Csum_j%20e%5E%7Bf_j%7D%0A%20%20\" alt=\"\n",
    "L_i = -log(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}})  = -f_{y_i} + log\\sum_j e^{f_j}\n",
    "\" />\n",
    "\n",
    "- <img src=\"https://i.upmath.me/svg/f_j\" alt=\"f_j\" /> is the j-th element of the vector of class scores <img src=\"https://i.upmath.me/svg/f\" alt=\"f\" />\n",
    "\n",
    "#### Information theory view:\n",
    "\n",
    "- The cross-entropy between a 'true' distribution <img src=\"https://i.upmath.me/svg/p\" alt=\"p\" /> and an estimated distribution <img src=\"https://i.upmath.me/svg/q\" alt=\"q\" /> is defined as:\n",
    "\n",
    "<img src=\"https://i.upmath.me/svg/%0A%20%20H(p%2Cq)%20%3D%20-%5Csum_x%20p(x)%20log%20q(x)%0A%20%20\" alt=\"\n",
    "H(p,q) = -\\sum_x p(x) log (q(x))\n",
    "\" />\n",
    "\n",
    "- The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities (<img src=\"https://i.upmath.me/svg/q%3D%5Cfrac%7Be%5E%7Bf_%7Byi%7D%7D%7D%7B%E2%88%91_j%20e%5E%7Bf_j%7D%7D\" alt=\"q=\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\" />\n",
    "  as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e.<img src=\"https://i.upmath.me/svg/p%3D%5B0%2C%E2%80%A61%2C%E2%80%A6%2C0%5D\" alt=\"p=[0,…1,…,0]\" /> contains a single <img src=\"https://i.upmath.me/svg/%201\" alt=\" 1\" /> at the <img src=\"https://i.upmath.me/svg/y_i\" alt=\"y_i\" /> <img src=\"https://i.upmath.me/svg/i-th\" alt=\"i-th\" /> position.)\n",
    "- Since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as <img src=\"https://i.upmath.me/svg/H(p%2Cq)%3DH(p)%2BDKL(p%7C%7Cq)\" alt=\"H(p,q)=H(p)+DKL(p||q)\" />, and the entropy of the delta function pp is zero, this is also equivalent to minimizing the KL divergence between the two distributions.\n",
    "\n",
    "#### Probabilistic Interpretation\n",
    "\n",
    "- We can see\n",
    "  <img src=\"https://i.upmath.me/svg/%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j%20e%5E%7Bf_j%7D%7D\" alt=\"\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\" /> as the normalized probability assigned to the correct label <img src=\"https://i.upmath.me/svg/y_i\" alt=\"y_i\" /> given the image <img src=\"https://i.upmath.me/svg/x_i\" alt=\"x_i\" />\n",
    "- In the probabilistic interpretation, we are therefore minimizing the negative log-likelihood of the correct class, which can be interpreted as performing *Maximum Likelihood Estimation* (MLE)\n",
    "- A nice feature of this view is that we can now also interpret the regularization term <img src=\"https://i.upmath.me/svg/R(W)\" alt=\"R(W)\" /> in the full loss function as coming from a Gaussian prior over the weight matrix <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" />, where instead of MLE we are performing the *Maximum a posteriori* (MAP) estimation\n",
    "\n",
    "### SVM vs Softmax\n",
    "\n",
    "#### Softmax classifier provides probabilities for each class\n",
    "\n",
    "#### SVM and Softmax are usually comparable\n",
    "\n",
    "- Compared to the Softmax classifier, the SVM is a more *local* objective\n",
    "  - The SVM does not care about the details of the individual scores: if they were instead [10, -100, -100] or [10, 9, 9] the SVM would be indifferent since the margin of 1 is satisfied and hence the loss is zero\n",
    "- The Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better\n",
    "\n",
    "## 3. Optimization\n",
    "\n",
    "### Optimization\n",
    "\n",
    "#### Random search\n",
    "\n",
    "- Try random weights and keep track of what works best\n",
    "\n",
    "#### Random local search\n",
    "\n",
    "- We will start out with a random <img src=\"https://i.upmath.me/svg/W\" alt=\"W\" />, generate random perturbations <img src=\"https://i.upmath.me/svg/%5CdeltaW\" alt=\"\\deltaW\" /> to it and if the loss at the perturbed <img src=\"https://i.upmath.me/svg/W%20%2B%20%5Cdelta%20W\" alt=\"W + \\delta W\" /> is lower, we will perform an update\n",
    "\n",
    "#### Following the gradient\n",
    "\n",
    "- It turns out that there is no need to randomly search for a good direction: we can compute the *best* direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descend\n",
    "\n",
    "### Computing the gradient\n",
    "\n",
    "#### Computing the gradient numerically with finite differences\n",
    "\n",
    "- Following the gradient formula we gave above, the code above iterates overall dimensions one by one, makes a small change `h` along that dimension, and calculates the partial derivative of the loss function along that dimension by seeing how much the function changed. The variable `grad` holds the full gradient in the end\n",
    "- it often works better to compute the numeric gradient using the **centered difference formula**\n",
    "- the numerical gradient has complexity linear in the number of parameters\n",
    "\n",
    "Computing the gradient with Calculus\n",
    "\n",
    "- ...\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "#### Mini-batch gradient descent\n",
    "\n",
    "- Compute the gradient over **batches** of the training data.\n",
    "- The gradient from a mini-batch is a good approximation of the gradient of the full objective\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- The extreme case of this is a setting where the mini-batch contains only a single example\n",
    "\n",
    "### 4. Backprop\n",
    "\n",
    "### 5. Linear Backprop\n",
    "\n",
    "### 6. Derivatives notes\n",
    "\n",
    "### 7. Efficient Backpropp\n",
    "\n",
    "### Related question/keyword/notes\n",
    "\n",
    "- CS229: Max margin property in SVM\n",
    "- https://stanford.edu/~boyd/cvxbook/\n",
    "- https://en.wikipedia.org/wiki/Subderivative\n",
    "- Following the gradient formula we gave above, the code above iterates overall dimensions one by one, makes a small change `h` along that dimension, and calculates the partial derivative of the loss function along that dimension by seeing how much the function changed. The variable `grad` holds the full gradient in the end\n",
    "- it often works better to compute the numeric gradient using the **centered difference formula**\n",
    "- the numerical gradient has complexity linear in the number of parameters\n",
    "\n",
    "Computing the gradient with Calculus\n",
    "\n",
    "- ...\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "#### Mini-batch gradient descent\n",
    "\n",
    "- Compute the gradient over **batches** of the training data.\n",
    "- The gradient from a mini-batch is a good approximation of the gradient of the full objective\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- The extreme case of this is a setting where the mini-batch contains only a single example\n",
    "\n",
    "### 4. Backprop\n",
    "\n",
    "### 5. Linear Backprop\n",
    "\n",
    "### 6. Derivatives notes\n",
    "\n",
    "### 7. Efficient Backpropp\n",
    "\n",
    "### Related question/keyword/notes\n",
    "\n",
    "- CS229: Max margin property in SVM\n",
    "- https://stanford.edu/~boyd/cvxbook/\n",
    "- https://en.wikipedia.org/wiki/Subderivative"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}